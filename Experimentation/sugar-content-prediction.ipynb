{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Experiment notebok is for using the Baseline models and the main model for feature extraction, and then performing a new downstream Regression Task i.e. Sugar COntent Prediction (Instead of Clustering Sugarbeet Fields into Disease and Healthy). \n",
    "\n",
    "This experiment is performed to check if the model performance changes, and how it changes, when adequate data for the downstream tasks is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "sys.path.append('/home/k64835/Master-Thesis-SITS')\n",
    "\n",
    "scripts_path = Path(\"../Data-Preprocessing/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Evaluation/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Modeling/\").resolve()\n",
    "sys.path.append(str(scripts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from scripts.data_visualiser import *\n",
    "from sklearn.manifold import TSNE \n",
    "from model_scripts.feature_extraction import *\n",
    "import torch.nn.functional as F\n",
    "from Experimentation.expt_scripts.sugarcontent_data_processing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_scripts.subpatch_extraction import *\n",
    "from Experimentation.expt_scripts.regression import *\n",
    "from scripts.data_loader import *\n",
    "from scripts.data_preprocessor import *\n",
    "from scripts.temporal_data_preprocessor import *\n",
    "from scripts.temporal_data_loader import *\n",
    "from scripts.temporal_visualiser import *\n",
    "from scripts.temporal_chanel_refinement import *\n",
    "from model_scripts.model_helper import *\n",
    "from model_scripts.dataset_creation import *\n",
    "from model_scripts.train_model_ae import *\n",
    "from model_scripts.model_visualiser import *\n",
    "from model_scripts.clustering import *\n",
    "from evaluation_scripts.evaluation_helper import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "import numpy as np\n",
    "import config as config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression on Flattened Images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prep: B10\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "Dimensions: (N, T, C, H, W) = (N, 7, 10, 64, 64)\n",
    "\n",
    "Here only the train data is used, since we have sugarcontent ground truths only for the train set. We divide this data into train and test again for calculating RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1228, 7, 10, 64, 64])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers_train, acquisition_dates_train, patch_tensor_train, visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'b10')\n",
    "patch_tensor_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sub-Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33128, 7, 10, 4, 4])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subpatches, train_subpatch_coords = non_overlapping_sliding_window(patch_tensor_train, field_numbers_train, patch_size=config.subpatch_size)\n",
    "train_subpatches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get properly formatted field numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1167136.0_1167138.0_24_24'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coord_fn = get_string_fielddata(train_subpatch_coords)\n",
    "train_coord_fn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the data for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33128, 1120)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subpatch_flat = train_subpatches.reshape(train_subpatches.size(0), -1).numpy()\n",
    "train_subpatch_flat.shape #(N, T * C * H * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugarcontent Data\n",
    "Get the sugarcontent values (y) for the train field numbers, and align them properly.\n",
    "PCA is performed if number of features > 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features, processed_field_numbers, sugar_contents = align_features_sugarcontent(train_subpatch_flat, train_coord_fn, config.sugarbeet_content_csv_path)\n",
    "processed_features[0].shape  #(T * C * H * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 11.6116\n"
     ]
    }
   ],
   "source": [
    "lr_model, lr_rmse, lr_preds = train_linear_regression(processed_features, sugar_contents, random_state=40)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(config.regression_linear_flat, 'wb') as file:\n",
    "#     pickle.dump(lr_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression on Histogram Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset prep: B10\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "Dimensions: (N, T, C, H, W) = (N, 7, 10, 64, 64)\n",
    "\n",
    "Here only the train data is used, since we have sugarcontent ground truths only for the train set. We divide this data into train and test again for calculating RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1228, 7, 10, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers_train, acquisition_dates_train, patch_tensor_train, visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'b10')\n",
    "patch_tensor_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sub-Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33128, 7, 10, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subpatches, train_subpatch_coords = non_overlapping_sliding_window(patch_tensor_train, field_numbers_train, patch_size=config.subpatch_size)\n",
    "train_subpatches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get properly formatted field numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1167136.0_1167138.0_24_24'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coord_fn = get_string_fielddata(train_subpatch_coords)\n",
    "train_coord_fn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction: Channel-wise histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33128, 350)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram_features_train = extract_channel_histograms(train_subpatches, bins=5)\n",
    "histogram_features_train.shape # (N, T * C * bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugarcontent Data\n",
    "Get the sugarcontent values (y) for the train field numbers, and align them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features, processed_field_numbers, sugar_contents = align_features_sugarcontent(histogram_features_train, train_coord_fn, config.sugarbeet_content_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 12.6863\n"
     ]
    }
   ],
   "source": [
    "lr_model, lr_rmse, lr_preds = train_linear_regression(processed_features, sugar_contents)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(config.regression_linear_hist, 'wb') as file:\n",
    "#     pickle.dump(lr_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction using 2D-Convolutional Autoencoders and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prep: B10\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "Dimensions: (N, T, C, H, W) = (N, 7, 10, 64, 64)\n",
    "\n",
    "Here only the train data is used, since we have sugarcontent ground truths only for the train set. We divide this data into train and test again for calculating RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1228, 7, 10, 64, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers_train, acquisition_dates_train, patch_tensor_train, visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'b10')\n",
    "patch_tensor_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sub-Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33128, 7, 10, 4, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subpatches, train_subpatch_coords = non_overlapping_sliding_window(patch_tensor_train, field_numbers_train, patch_size=config.subpatch_size)\n",
    "train_subpatches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get properly formatted field numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1167136.0_1167138.0_24_24'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coord_fn = get_string_fielddata(train_subpatch_coords)\n",
    "train_coord_fn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = create_data_loader(train_subpatches, train_coord_fn, batch_size=config.ae_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved 2D Conv Autoencoder\n",
    "Redefine the architecture because the object needs to be created to load saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels, time_steps, latent_size, patch_size):\n",
    "        super(Conv2DAutoencoder, self).__init__()\n",
    "\n",
    "        self.time_steps = time_steps\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # --- Encoder (2D Convolutions, treating time steps as channels) ---\n",
    "        self.conv1 = nn.Conv2d(in_channels * time_steps, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # --- Fully Connected Latent Space ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * patch_size * patch_size, 512)\n",
    "        self.fc2 = nn.Linear(512, latent_size)\n",
    "\n",
    "        # --- Decoder (Fully Connected) ---\n",
    "        self.fc3 = nn.Linear(latent_size, 512)\n",
    "        self.fc4 = nn.Linear(512, 256 * patch_size * patch_size)\n",
    "\n",
    "        # --- 2D Deconvolutions ---\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, in_channels * time_steps, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        b, c, t, h, w = x.shape\n",
    "        x = x.reshape(b, c * t, h, w)      # Imp: Time steps as additional channels (B, C * D, H, W)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # --- Flatten and Fully Connected ---\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        x = F.relu(self.fc3(z))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        # --- 2D Deconvolutions ---\n",
    "        x = x.view(b, 256, h, w)        \n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = self.deconv3(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # --- Reshape to B x C x D x H x W ---\n",
    "        x_reconstructed = x.view(b, self.in_channels, self.time_steps, h, w) \n",
    "\n",
    "        return z, x_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2DAutoencoder(\n",
       "  (conv1): Conv2d(70, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=4096, bias=True)\n",
       "  (deconv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (deconv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (deconv3): ConvTranspose2d(64, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim=32\n",
    "channels = 10\n",
    "time_steps = 7\n",
    "device = 'cuda'\n",
    "trained_model = Conv2DAutoencoder(channels, time_steps, latent_dim, config.subpatch_size)\n",
    "\n",
    "with open(config.ae_2D_path, 'rb') as file:\n",
    "    trained_model = pickle.load(file)\n",
    "\n",
    "device = torch.device(device)  \n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features/Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_coord_dl = extract_features_ae(trained_model, dataloader_train, temp_embed_pixel=False, device=device)\n",
    "train_features = train_features.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugarcontent Data\n",
    "Get the sugarcontent values (y) for the train field numbers, and align them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features, processed_field_numbers, sugar_contents = align_features_sugarcontent(train_features, train_coord_dl, config.sugarbeet_content_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50662, 32)\n"
     ]
    }
   ],
   "source": [
    "processed_features = np.stack([f.numpy() for f in processed_features])\n",
    "print(processed_features.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 12.3439\n"
     ]
    }
   ],
   "source": [
    "lr_model, lr_rmse, lr_preds = train_linear_regression(processed_features, sugar_contents, random_state=41)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(config.regression_linear_2dconv, 'wb') as file:\n",
    "#     pickle.dump(lr_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction using 3D-Convolutional Autoencoders (with Temporal Embeddings) and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prep: B10\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "Dimensions: (N, T, C, H, W) = (N, 7, 10, 64, 64)\n",
    "\n",
    "Here only the train data is used, since we have sugarcontent ground truths only for the train set. We divide this data into train and test again for calculating RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1228, 7, 10, 64, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers_train, acquisition_dates_train, date_emb_train, patch_tensor_train, images_visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'b10_add', method='sin-cos')\n",
    "patch_tensor_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sub-Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33128, 7, 10, 4, 4])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subpatches, train_subpatch_coords, train_subpatch_date_emb = non_overlapping_sliding_window_with_date_emb(patch_tensor_train, field_numbers_train, date_emb_train, patch_size=config.subpatch_size)\n",
    "train_subpatches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get properly formatted field numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1167136.0_1167138.0_24_24'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coord_fn = get_string_fielddata(train_subpatch_coords)\n",
    "train_coord_fn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = create_data_loader_mae(train_subpatches, train_coord_fn, train_subpatch_date_emb, mae=False, batch_size=config.ae_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved 3D Conv Autoencoder trained with temporal data\n",
    "Redefine the architecture because the object needs to be created to load saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels, time_steps, latent_size, patch_size):\n",
    "        super(Conv3DAutoencoder, self).__init__()\n",
    "\n",
    "        self.time_steps = time_steps\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # --- Encoder (3D Convolutions) ---\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # --- Fully Connected Latent Space ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * patch_size * patch_size * time_steps, 512)   \n",
    "        self.fc2 = nn.Linear(512, latent_size)\n",
    "\n",
    "        # --- Decoder (Fully Connected) ---\n",
    "        self.fc3 = nn.Linear(latent_size, 512)\n",
    "        self.fc4 = nn.Linear(512, 256 * patch_size * patch_size * time_steps)\n",
    "\n",
    "        # --- 3D Deconvolutions (Transpose convolutions) ---\n",
    "        self.unflatten = nn.Unflatten(1, (256, time_steps, patch_size, patch_size))\n",
    "        self.deconv1 = nn.ConvTranspose3d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose3d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose3d(64, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # --- Temporal embedding projection to match channels (needed for alignment) ---\n",
    "        self.temb_proj = nn.Conv3d(2, in_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, date_embeddings):\n",
    "\n",
    "        # --- Date embedding processing ---\n",
    "        # Convert the date embeddings to the shape (B, 2, 7, 4, 4)\n",
    "        if not isinstance(date_embeddings, torch.Tensor):\n",
    "            date_embeddings_tensor = torch.tensor(date_embeddings, dtype=torch.float32).to(x.device)    # Shape: (B, 7, 2)\n",
    "        date_embeddings_tensor = date_embeddings_tensor.permute(0, 2, 1)                                # Shape: (B, 2, 7)\n",
    "        date_embeddings_tensor = date_embeddings_tensor.unsqueeze(-1).unsqueeze(-1)                     # Shape: (B, 2, 7, 1, 1)\n",
    "        date_embeddings_tensor = date_embeddings_tensor.expand(-1, -1, -1, x.shape[3], x.shape[4])      # Shape: (B, 2, 7, 4, 4)\n",
    "\n",
    "        # Project the date embeddings to match the channels\n",
    "        date_embeddings_tensor = self.temb_proj(date_embeddings_tensor)                                 # Shape: (B, 10, 7, 4, 4)\n",
    "        # print('x shape before time embedding:',x.shape)\n",
    "        # print('time embeddings:',date_embeddings_tensor.shape)\n",
    "        \n",
    "        # --- Add date embeddings to the input tensor ---\n",
    "        x = x + date_embeddings_tensor                                                                  # Shape: (B, 10, 7, 4, 4)\n",
    "        # print('x shape after time embedding',x.shape)\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # --- Flatten and Fully Connected ---\n",
    "        b, c, t, h, w = x.shape                 # (B, C, T, H, W)\n",
    "        x = self.flatten(x)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)                         # Bottleneck    \n",
    "\n",
    "        # --- Decoder ---\n",
    "        x = F.relu(self.fc3(z))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        # --- Reshape and 3D Deconvolutions ---\n",
    "        x = self.unflatten(x)                   # (B, C, H, W, T)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x_reconstructed = self.deconv3(x)       # Reconstruction\n",
    "\n",
    "        return z, x_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3DAutoencoder(\n",
       "  (conv1): Conv3d(10, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=28672, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=28672, bias=True)\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(256, 7, 4, 4))\n",
       "  (deconv1): ConvTranspose3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (deconv2): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (deconv3): ConvTranspose3d(64, 10, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (temb_proj): Conv3d(2, 10, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim=32\n",
    "in_channels = 12\n",
    "momentum = 0.9\n",
    "time_steps = 7\n",
    "device = 'cuda'\n",
    "model = Conv3DAutoencoder(in_channels, time_steps, latent_dim, config.subpatch_size)\n",
    "\n",
    "with open(config.ae_3d_TEadd_path, 'rb') as file:\n",
    "    trained_model = pickle.load(file)\n",
    "\n",
    "device = torch.device(device)  \n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features/Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_coord_dl = extract_features_ae(trained_model, dataloader_train, temp_embed_pixel=True, device=device)\n",
    "train_features = train_features.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugarcontent Data\n",
    "Get the sugarcontent values (y) for the train field numbers, and align them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features, processed_field_numbers, sugar_contents = align_features_sugarcontent(train_features, train_coord_dl, config.sugarbeet_content_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50662, 32)\n"
     ]
    }
   ],
   "source": [
    "processed_features = np.stack([f.numpy() for f in processed_features])\n",
    "print(processed_features.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 12.3351\n"
     ]
    }
   ],
   "source": [
    "lr_model, lr_rmse, lr_preds = train_linear_regression(processed_features, sugar_contents, random_state=43)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(config.regression_linear_3dconv, 'wb') as file:\n",
    "#     pickle.dump(lr_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
