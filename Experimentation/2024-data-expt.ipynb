{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "sys.path.append('/home/k64835/Master-Thesis-SITS')\n",
    "\n",
    "scripts_path = Path(\"../Data-Preprocessing/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Evaluation/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Modeling/\").resolve()\n",
    "sys.path.append(str(scripts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from scripts.data_visualiser import *\n",
    "from sklearn.manifold import TSNE \n",
    "from model_scripts.feature_extraction import *\n",
    "from model_scripts.executions import *\n",
    "import torch.nn.functional as F\n",
    "from Experimentation.expt_scripts.sugarcontent_data_processing import *\n",
    "from Experimentation.expt_scripts.expt_plots import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_scripts.subpatch_extraction import *\n",
    "from Experimentation.expt_scripts.regression import *\n",
    "from scripts.data_loader import *\n",
    "from scripts.data_preprocessor import *\n",
    "from scripts.temporal_data_preprocessor import *\n",
    "from scripts.temporal_data_loader import *\n",
    "from scripts.temporal_visualiser import *\n",
    "from scripts.temporal_chanel_refinement import *\n",
    "from model_scripts.model_helper import *\n",
    "from model_scripts.dataset_creation import *\n",
    "from model_scripts.train_model_ae import *\n",
    "from model_scripts.model_visualiser import *\n",
    "from model_scripts.clustering import *\n",
    "from evaluation_scripts.evaluation_helper import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "import numpy as np\n",
    "import config as config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep: B10\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "\n",
    "Dimensions: (N, T, C, H, W) = (N, 7, 10, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "\n",
    "# preprocessing_pipeline.run_temporal_patch_save_pipeline(type='train')\n",
    "# preprocessing_pipeline.run_temporal_patch_save_pipeline(type='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1491, 4, 10, 64, 64]), torch.Size([33, 4, 10, 64, 64]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "\n",
    "field_numbers_train, acquisition_dates_train, date_emb_train, patch_tensor_train, images_visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'b10_add', method='sin-cos',data_year='2024')\n",
    "field_numbers_eval, acquisition_dates_eval, date_emb_eval, patch_tensor_eval, images_visualisation_eval = preprocessing_pipeline.get_processed_temporal_cubes('eval', 'b10_add', method='sin-cos', data_year='2024')\n",
    "patch_tensor_train.shape, patch_tensor_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sub-Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50529, 4, 10, 4, 4]), torch.Size([1213, 4, 10, 4, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subpatches, train_subpatch_coords, train_subpatch_date_emb = non_overlapping_sliding_window_with_date_emb(patch_tensor_train, field_numbers_train, date_emb_train, patch_size=config.subpatch_size)\n",
    "eval_subpatches, eval_subpatch_coords, eval_subpatch_date_emb = non_overlapping_sliding_window_with_date_emb(patch_tensor_eval, field_numbers_eval, date_emb_eval, patch_size=config.subpatch_size)\n",
    "train_subpatches.shape, eval_subpatches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get field numbers and co-ordinates as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1615767.0_12_20', 1213, 50529)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coord_dataloader = get_string_fielddata(train_subpatch_coords)\n",
    "eval_coord_dataloader = get_string_fielddata(eval_subpatch_coords)\n",
    "eval_coord_dataloader[0], len(eval_coord_dataloader), len(train_coord_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Unlabeled data into 'train' and 'test' and create  Data Loaders\n",
    "\n",
    "\n",
    "The data loader function for MAE is used since it is designed to take temporal encodings additionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subpatches_dl, test_subpatches, train_field_numbers, test_field_numbers, train_date_embeddings, test_date_embeddings = train_test_split(\n",
    "    train_subpatches, train_coord_dataloader, train_subpatch_date_emb, test_size=1-config.ae_train_test_ratio, random_state=42\n",
    ")\n",
    "\n",
    "dataloader_train = create_data_loader_mae(train_subpatches_dl, train_field_numbers, train_date_embeddings, mae=False, batch_size=config.ae_batch_size, shuffle=True)\n",
    "dataloader_test = create_data_loader_mae(test_subpatches, test_field_numbers, test_date_embeddings, mae=False, batch_size=config.ae_batch_size, shuffle=False)\n",
    "dataloader_eval = create_data_loader_mae(eval_subpatches, eval_coord_dataloader, eval_subpatch_date_emb, mae=False, batch_size=config.ae_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep: Conv3D Autoencoder with Temporal Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved 3D Conv Autoencoder trained with temporal data\n",
    "Redefine the architecture because the object needs to be created to load saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DAutoencoder_Time_Addition(nn.Module):\n",
    "    def __init__(self, in_channels, time_steps, latent_size, patch_size):\n",
    "        super(Conv3DAutoencoder_Time_Addition, self).__init__()\n",
    "\n",
    "        self.time_steps = time_steps\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # --- Encoder (3D Convolutions) ---\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # --- Fully Connected Latent Space ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * patch_size * patch_size * time_steps, 512)   \n",
    "        self.fc2 = nn.Linear(512, latent_size)\n",
    "\n",
    "        # --- Decoder (Fully Connected) ---\n",
    "        self.fc3 = nn.Linear(latent_size, 512)\n",
    "        self.fc4 = nn.Linear(512, 256 * patch_size * patch_size * time_steps)\n",
    "\n",
    "        # --- 3D Deconvolutions (Transpose convolutions) ---\n",
    "        self.unflatten = nn.Unflatten(1, (256, time_steps, patch_size, patch_size))\n",
    "        self.deconv1 = nn.ConvTranspose3d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose3d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose3d(64, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # --- Temporal embedding projection to match channels (needed for alignment) ---\n",
    "        self.temb_proj = nn.Conv3d(2, in_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, date_embeddings):\n",
    "\n",
    "        # --- Date embedding processing ---\n",
    "        # Convert the date embeddings to the shape (B, 2, 7, 4, 4)\n",
    "        if not isinstance(date_embeddings, torch.Tensor):\n",
    "            date_embeddings_tensor = torch.tensor(date_embeddings, dtype=torch.float32).to(x.device)    # Shape: (B, 7, 2)\n",
    "        date_embeddings_tensor = date_embeddings_tensor.permute(0, 2, 1)                                # Shape: (B, 2, 7)\n",
    "        date_embeddings_tensor = date_embeddings_tensor.unsqueeze(-1).unsqueeze(-1)                     # Shape: (B, 2, 7, 1, 1)\n",
    "        date_embeddings_tensor = date_embeddings_tensor.expand(-1, -1, -1, x.shape[3], x.shape[4])      # Shape: (B, 2, 7, 4, 4)\n",
    "\n",
    "        # Project the date embeddings to match the channels\n",
    "        date_embeddings_tensor = self.temb_proj(date_embeddings_tensor)                                 # Shape: (B, 10, 7, 4, 4)\n",
    "        # print('x shape before time embedding:',x.shape)\n",
    "        # print('time embeddings:',date_embeddings_tensor.shape)\n",
    "        \n",
    "        # --- Add date embeddings to the input tensor ---\n",
    "        x = x + date_embeddings_tensor                                                                  # Shape: (B, 10, 7, 4, 4)\n",
    "        # print('x shape after time embedding',x.shape)\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # --- Flatten and Fully Connected ---\n",
    "        b, c, t, h, w = x.shape                 # (B, C, T, H, W)\n",
    "        x = self.flatten(x)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        z = self.fc2(x)                         # Bottleneck    \n",
    "\n",
    "        # --- Decoder ---\n",
    "        x = F.relu(self.fc3(z))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        # --- Reshape and 3D Deconvolutions ---\n",
    "        x = self.unflatten(x)                   # (B, C, H, W, T)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x_reconstructed = self.deconv3(x)       # Reconstruction\n",
    "\n",
    "        return z, x_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "epochs = 50\n",
    "momentum=0.9\n",
    "lr = 0.001\n",
    "vae_lr=0.001\n",
    "latent_dim = 32\n",
    "channels = 10\n",
    "time_steps = 4\n",
    "optimizer = 'Adam'\n",
    "vae_optimizer = 'Adam'\n",
    "patch_size = config.subpatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  3D_AE_temporal_addition_2024  trained\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"3D_AE_temporal_addition_2024\"]\n",
    "model_objs = [Conv3DAutoencoder_Time_Addition]  \n",
    "train_loss = {}\n",
    "test_loss = {}\n",
    "metrics = {}\n",
    "\n",
    "for name, obj in zip(model_names, model_objs):\n",
    "    avg_train_loss, avg_test_loss, avg_metrics = train_model_multiple_runs_with_metrics(\n",
    "        model_name=name,\n",
    "        model_class=obj,\n",
    "        dataloader_train=dataloader_train,\n",
    "        dataloader_test=dataloader_test,\n",
    "        dataloader_eval=dataloader_eval,\n",
    "        channels=channels,\n",
    "        timestamps=time_steps,\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        lr=lr,\n",
    "        vae_lr=vae_lr,\n",
    "        vae_optimizer=vae_optimizer,\n",
    "        momentum=momentum,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        output_dir=config.results_json_path\n",
    "    )\n",
    "    print(\"Model \",name,\" trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
