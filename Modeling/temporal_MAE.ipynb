{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "sys.path.append('/home/k64835/Master-Thesis-SITS')\n",
    "\n",
    "scripts_path = Path(\"../Data-Preprocessing/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Evaluation/\").resolve()\n",
    "sys.path.append(str(scripts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from scripts.data_visualiser import *\n",
    "from sklearn.manifold import TSNE \n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_scripts.subpatch_extraction import *\n",
    "from scripts.data_loader import *\n",
    "from scripts.data_preprocessor import *\n",
    "from scripts.temporal_data_preprocessor import *\n",
    "from scripts.temporal_data_loader import *\n",
    "from scripts.temporal_visualiser import *\n",
    "from scripts.temporal_chanel_refinement import *\n",
    "from model_scripts.model_helper import *\n",
    "from model_scripts.dataset_creation import *\n",
    "from model_scripts.train_model_ae import *\n",
    "from model_scripts.model_visualiser import *\n",
    "from model_scripts.clustering import *\n",
    "from model_scripts.train_model_dcec import *\n",
    "from evaluation_scripts.evaluation_helper import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "import numpy as np\n",
    "import config as config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep: B10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-processed data\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "\n",
    "Dimensions: (N, T, C, H, W) = (N, 7, 10, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2425, 7, 10, 64, 64]), torch.Size([48, 7, 10, 64, 64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers_train, acquisition_dates_train, patch_tensor_train, images_visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'allbands')\n",
    "field_numbers_eval, acquisition_dates_eval, patch_tensor_eval, images_visualisation_eval = preprocessing_pipeline.get_processed_temporal_cubes('eval', 'allbands')\n",
    "patch_tensor_train.shape, patch_tensor_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def patchify(images, patch_size=4):\n",
    "\n",
    "    B, T, C, H, W = images.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"Image dimensions must be divisible by patch_size\"\n",
    "    \n",
    "    patches = images.unfold(3, patch_size, patch_size).unfold(4, patch_size, patch_size)\n",
    "    patches = patches.contiguous().view(B, T, C, -1, patch_size, patch_size)  # (B, T, C, num_patches, patch_size, patch_size)\n",
    "\n",
    "    num_patches = patches.shape[3]\n",
    "\n",
    "    # Create a valid patch mask (1 for valid patches, 0 for invalid patches)\n",
    "    valid_patch_mask = (patches.sum(dim=[4, 5]) != 0).float()  # Shape: (B, T, num_patches)\n",
    "    \n",
    "    return patches, valid_patch_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches shape: torch.Size([2425, 7, 10, 256, 4, 4])\n",
      "Valid patch mask shape: torch.Size([2425, 7, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "images = patch_tensor_train  # Example input tensor with shape (B=4, T=7, C=10, H=64, W=64)\n",
    "patch_size = 4\n",
    "\n",
    "patches, valid_patch_mask = patchify(images, patch_size)\n",
    "\n",
    "print(\"Patches shape:\", patches.shape)  # Should print: (B, T, C, num_patches, patch_size, patch_size)\n",
    "print(\"Valid patch mask shape:\", valid_patch_mask.shape)  # Should print: (B, T, C, num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding function (sinusoidal)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Spatial Positional Encoding (2D)\n",
    "        self.spatial_pos_encoding = self.get_2d_positional_encoding(num_patches, embed_dim)\n",
    "        \n",
    "        # Temporal Positional Encoding (1D)\n",
    "        self.temporal_pos_encoding = self.get_1d_positional_encoding(max_len, embed_dim)\n",
    "    \n",
    "    def get_2d_positional_encoding(self, num_patches, embed_dim):\n",
    "        # Get the 2D positional encoding (sine/cosine)\n",
    "        grid_size = int(math.sqrt(num_patches))  # Assuming square grid\n",
    "        \n",
    "        # Generate meshgrid for spatial positions\n",
    "        y_pos, x_pos = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size))\n",
    "        y_pos = y_pos.flatten()\n",
    "        x_pos = x_pos.flatten()\n",
    "        \n",
    "        # Create positional encoding matrix for each spatial position\n",
    "        pos = torch.stack([x_pos, y_pos], dim=-1).unsqueeze(0)  # shape: (1, num_patches, 2)\n",
    "        \n",
    "        # Apply sine/cosine function\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * torch.arange(0, embed_dim, 2).float()) / embed_dim)\n",
    "        angle_rads = pos @ angle_rates  # (1, num_patches, embed_dim // 2)\n",
    "        \n",
    "        # Apply sin and cos functions\n",
    "        pos_encoding = torch.cat([torch.sin(angle_rads), torch.cos(angle_rads)], dim=-1)\n",
    "        return pos_encoding\n",
    "    \n",
    "    def get_1d_positional_encoding(self, max_len, embed_dim):\n",
    "        # Generate 1D positional encodings (sine/cosine)\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "        angle_rads = position * div_term  # (max_len, embed_dim // 2)\n",
    "        \n",
    "        # Apply sin and cos functions\n",
    "        pos_encoding = torch.cat([torch.sin(angle_rads), torch.cos(angle_rads)], dim=-1)  # shape: (max_len, embed_dim)\n",
    "        return pos_encoding.unsqueeze(0)  # shape: (1, max_len, embed_dim)\n",
    "    \n",
    "    def forward(self, spatial_indices, temporal_indices):\n",
    "        # spatial_indices: (batch_size, num_patches)\n",
    "        # temporal_indices: (batch_size, T)\n",
    "        \n",
    "        spatial_encoding = self.spatial_pos_encoding[:, spatial_indices]  # shape: (batch_size, num_patches, embed_dim)\n",
    "        temporal_encoding = self.temporal_pos_encoding[:, temporal_indices]  # shape: (batch_size, T, embed_dim)\n",
    "        \n",
    "        return spatial_encoding, temporal_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Block (Transformer Block)\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (num_patches, batch_size, embed_dim)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.ln1(attn_out + x)  # Add & Norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(ffn_out + x)  # Add & Norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Encoder\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size=4, img_size=64, num_channels=10, max_len=5000):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Patch embedding layer\n",
    "        self.patch_embed = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim=embed_dim, num_patches=num_patches, max_len=max_len)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [ViTBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, temporal_indices):\n",
    "        # x shape: (B, T, C, H, W) --> (B, T, num_patches, embed_dim)\n",
    "        B, T, C, H, W = x.shape\n",
    "        patches = self.patch_embed(x.view(B * T, C, H, W))  # (B * T, embed_dim, H/patch_size, W/patch_size)\n",
    "        patches = patches.flatten(2).transpose(1, 2)  # (B * T, num_patches, embed_dim)\n",
    "        \n",
    "        # Spatial and Temporal Positional Encoding\n",
    "        spatial_indices = torch.arange(patches.size(1)).unsqueeze(0).expand(B * T, -1).to(x.device)\n",
    "        spatial_encoding, temporal_encoding = self.pos_encoder(spatial_indices, temporal_indices)\n",
    "        \n",
    "        # Add positional encodings\n",
    "        patches = patches + spatial_encoding + temporal_encoding.unsqueeze(1)  # (B * T, num_patches, embed_dim)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            patches = block(patches)\n",
    "        \n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Decoder\n",
    "class ViTDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size=16, img_size=64, num_channels=10):\n",
    "        super(ViTDecoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # Transformer decoder layers\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [ViTBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # Decoder output to image reconstruction\n",
    "        self.fc = nn.Linear(embed_dim, num_channels * patch_size * patch_size)\n",
    "\n",
    "        # Reshape into image space\n",
    "        self.reshaper = nn.Unflatten(2, (num_channels, patch_size, patch_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B * T, num_patches, embed_dim)\n",
    "        B, T, num_patches, embed_dim = x.shape\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Reshape back to image patches\n",
    "        x = self.fc(x)\n",
    "        x = self.reshaper(x)  # (B * T, num_patches, C, patch_size, patch_size)\n",
    "        x = x.view(B, T, num_patches, -1, self.patch_size, self.patch_size)  # (B, T, num_patches, C, patch_size, patch_size)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAEViT Model (Masked Autoencoder with Vision Transformer)\n",
    "class MAEViT(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, ff_dim=1024, num_layers=6, num_patches=10, patch_size=16, img_size=64, num_channels=10):\n",
    "        super(MAEViT, self).__init__()\n",
    "        self.encoder = ViTEncoder(embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels)\n",
    "        self.decoder = ViTDecoder(embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels)\n",
    "\n",
    "    def forward(self, x, temporal_indices):\n",
    "        encoded_patches = self.encoder(x, temporal_indices)  # (B * T, num_patches, embed_dim)\n",
    "        reconstructed = self.decoder(encoded_patches)  # (B, T, num_patches, C, patch_size, patch_size)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got input (256), mat (256x2), vec (128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m temporal_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(T)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Example temporal indices\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMAEViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m reconstructed_images \u001b[38;5;241m=\u001b[39m model(images, temporal_indices)\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mMAEViT.__init__\u001b[0;34m(self, embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(MAEViT, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mViTEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m ViTDecoder(embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels)\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mViTEncoder.__init__\u001b[0;34m(self, embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(num_channels, embed_dim, kernel_size\u001b[38;5;241m=\u001b[39mpatch_size, stride\u001b[38;5;241m=\u001b[39mpatch_size)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Positional encoding\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Transformer encoder layers\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     16\u001b[0m     [ViTBlock(embed_dim, num_heads, ff_dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, num_patches, embed_dim, max_len)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28msuper\u001b[39m(PositionalEncoding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Spatial Positional Encoding (2D)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_pos_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_2d_positional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Temporal Positional Encoding (1D)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_pos_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_1d_positional_encoding(max_len, embed_dim)\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mPositionalEncoding.get_2d_positional_encoding\u001b[0;34m(self, num_patches, embed_dim)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply sine/cosine function\u001b[39;00m\n\u001b[1;32m     25\u001b[0m angle_rates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m10000\u001b[39m, (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, embed_dim, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()) \u001b[38;5;241m/\u001b[39m embed_dim)\n\u001b[0;32m---> 26\u001b[0m angle_rads \u001b[38;5;241m=\u001b[39m \u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mangle_rates\u001b[49m  \u001b[38;5;66;03m# (1, num_patches, embed_dim // 2)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply sin and cos functions\u001b[39;00m\n\u001b[1;32m     29\u001b[0m pos_encoding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39msin(angle_rads), torch\u001b[38;5;241m.\u001b[39mcos(angle_rads)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got input (256), mat (256x2), vec (128)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "B = 4  # Batch size\n",
    "T = 7  # Number of temporal images (7 acquisition dates)\n",
    "C = 10  # Number of channels\n",
    "H = 64  # Image height\n",
    "W = 64  # Image width\n",
    "\n",
    "images = torch.randn(B, T, C, H, W)  # Example batch of satellite images\n",
    "temporal_indices = torch.arange(T).unsqueeze(0).expand(B, -1)  # Example temporal indices\n",
    "\n",
    "# Create model\n",
    "model = MAEViT(embed_dim=256, num_heads=8, ff_dim=1024, num_layers=6, num_patches=256, patch_size=16, img_size=64, num_channels=10)\n",
    "\n",
    "# Forward pass\n",
    "reconstructed_images = model(images, temporal_indices)\n",
    "\n",
    "print(f\"Reconstructed Images shape: {reconstructed_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
