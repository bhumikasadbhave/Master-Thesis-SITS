{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "sys.path.append('/home/k64835/Master-Thesis-SITS')\n",
    "\n",
    "scripts_path = Path(\"../Data-Preprocessing/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Evaluation/\").resolve()\n",
    "sys.path.append(str(scripts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k64835/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/k64835/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/k64835/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from scripts.data_visualiser import *\n",
    "from sklearn.manifold import TSNE \n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_scripts.subpatch_extraction import *\n",
    "from scripts.data_loader import *\n",
    "from scripts.data_preprocessor import *\n",
    "from scripts.temporal_data_preprocessor import *\n",
    "from scripts.temporal_data_loader import *\n",
    "from scripts.temporal_visualiser import *\n",
    "from scripts.temporal_chanel_refinement import *\n",
    "from model_scripts.model_helper import *\n",
    "from model_scripts.dataset_creation import *\n",
    "from model_scripts.train_model_ae import *\n",
    "from model_scripts.model_visualiser import *\n",
    "from model_scripts.clustering import *\n",
    "from model_scripts.train_model_dcec import *\n",
    "from model_scripts.pos_embed import *\n",
    "from model_scripts.train_mae import *\n",
    "from model_scripts.MaskedAutoencoderViT import *\n",
    "from model_scripts.MaskedAutoencoderViT_7ts import *\n",
    "from evaluation_scripts.evaluation_helper import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from evaluation_scripts.patch_evaluation_helper import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "import numpy as np\n",
    "import datetime\n",
    "import config as config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep: B10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers, acquisition_dates, patch_tensor, images_visualisation_train = preprocessing_pipeline.get_processed_temporal_cube3('train', 'rgb')\n",
    "field_numbers_eval, acquisition_dates_eval, patch_tensor_eval, images_visualisation_eval = preprocessing_pipeline.get_processed_temporal_cube3('eval', 'rgb')\n",
    "patch_tensor.shape, patch_tensor_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = process_timestamps(field_numbers, acquisition_dates)\n",
    "eval_timestamps = process_timestamps(field_numbers_eval, acquisition_dates_eval)\n",
    "len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor, test_tensor, train_field_numbers, test_field_numbers, timestamps_train, timestamps_test = train_test_split(\n",
    "    patch_tensor, field_numbers, timestamps, test_size=1-config.ae_train_test_ratio, random_state=42\n",
    ")\n",
    "\n",
    "dataloader_train = create_data_loader_mae(train_tensor, train_field_numbers, timestamps_train, batch_size=config.ae_batch_size, shuffle=True)\n",
    "dataloader_test = create_data_loader_mae(test_tensor, test_field_numbers, timestamps_test, batch_size=config.ae_batch_size, shuffle=False)\n",
    "dataloader_eval = create_data_loader_mae(patch_tensor_eval, field_numbers_eval, eval_timestamps, batch_size=config.ae_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = get_augmentation_transforms()\n",
    "augmented_dataloader = create_augmented_data_loader(dataloader_train, augmentations)\n",
    "len(augmented_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE Script -------> Runnnnnnnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "momentum=0.9\n",
    "latent_dim=32\n",
    "channels = 3\n",
    "time_steps = 3\n",
    "optimizer = 'Adam'\n",
    "patch_size = config.subpatch_size\n",
    "masking_ratio = 0.25\n",
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(\n",
    "        # img_size=64,\n",
    "        patch_size=patch_size, embed_dim=768, depth=6, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=2, decoder_num_heads=16,\n",
    "        mlp_ratio=2, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, train_losses, test_losses = train_model_mae(model, dataloader_train, dataloader_test, epochs=epochs, masking_ratio=masking_ratio, optimizer=optimizer, lr=lr, momentum=momentum, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_log_scale(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use trained Encoder part to get the features for train, test and evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_fno = extract_latent_features_mae(trained_model, dataloader_train, device)\n",
    "test_features, test_fno = extract_latent_features_mae(trained_model, dataloader_test, device)\n",
    "eval_features, eval_fno = extract_latent_features_mae(trained_model, dataloader_eval, device)\n",
    "\n",
    "train_features = train_features.cpu()\n",
    "test_features = test_features.cpu()\n",
    "eval_features = eval_features.cpu()\n",
    "\n",
    "combined_train_features = torch.cat((train_features, test_features), dim=0)\n",
    "combined_train_coords = train_fno + test_fno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = train_kmeans_patches(combined_train_features, n_clusters=2, random_state=109)\n",
    "\n",
    "train_patch_predictions = kmeans.predict(combined_train_features.reshape(combined_train_features.size(0), -1).numpy().astype(np.float32))\n",
    "eval_patch_predictions = kmeans.predict(eval_features.reshape(eval_features.size(0), -1).numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_patch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, precision, recall, f1, f2 = get_clustering_accuracy(field_numbers_eval, eval_patch_predictions, config.labels_path)\n",
    "print(\"Accuracy:\",acc)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"F1-score:\",f1)\n",
    "print(\"F2-score\",f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_temporal_reconstructions(trained_model, dataloader_eval, device, num_images=5, T=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2425, 7, 3, 64, 64]), torch.Size([48, 7, 3, 64, 64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers, acquisition_dates, patch_tensor, images_visualisation_train = preprocessing_pipeline.get_processed_temporal_cubes('train', 'rgb')\n",
    "field_numbers_eval, acquisition_dates_eval, patch_tensor_eval, images_visualisation_eval = preprocessing_pipeline.get_processed_temporal_cubes('eval', 'rgb')\n",
    "patch_tensor.shape, patch_tensor_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2425"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps = process_timestamps7(field_numbers, acquisition_dates)\n",
    "eval_timestamps = process_timestamps7(field_numbers_eval, acquisition_dates_eval)\n",
    "len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k64835/Master-Thesis-SITS/Modeling/model_scripts/dataset_creation.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).permute(0, 2, 1, 3, 4)   # (N, T, C, H, W) -> (N, C, T, H, W)\n"
     ]
    }
   ],
   "source": [
    "train_tensor, test_tensor, train_field_numbers, test_field_numbers, timestamps_train, timestamps_test = train_test_split(\n",
    "    patch_tensor, field_numbers, timestamps, test_size=1-config.ae_train_test_ratio, random_state=42\n",
    ")\n",
    "\n",
    "dataloader_train = create_data_loader_mae(train_tensor, train_field_numbers, timestamps_train, batch_size=config.ae_batch_size, shuffle=True)\n",
    "dataloader_test = create_data_loader_mae(test_tensor, test_field_numbers, timestamps_test, batch_size=config.ae_batch_size, shuffle=False)\n",
    "dataloader_eval = create_data_loader_mae(patch_tensor_eval, field_numbers_eval, eval_timestamps, batch_size=config.ae_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Inputs Shape: torch.Size([64, 3, 7, 64, 64])\n",
      "Dates: torch.Size([64, 7, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_inputs, batch_field_numbers, timestamps in dataloader_train:\n",
    "    print(\"Batch Inputs Shape:\", batch_inputs.shape)  \n",
    "    # print(\"Batch Field Numbers:\", batch_field_numbers)\n",
    "    print(\"Dates:\", timestamps.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "momentum=0.9\n",
    "latent_dim=32\n",
    "channels = 3\n",
    "time_steps = 7\n",
    "optimizer = 'Adam'\n",
    "patch_size = config.subpatch_size\n",
    "masking_ratio = 0.75\n",
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT_7ts(\n",
    "        # img_size=64,\n",
    "        patch_size=patch_size, embed_dim=768, depth=6, num_heads=6,\n",
    "        decoder_embed_dim=512, decoder_depth=2, decoder_num_heads=8,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, train_losses, test_losses = train_model_mae(model, dataloader_train, dataloader_test, epochs=epochs, masking_ratio=masking_ratio, optimizer=optimizer, lr=lr, momentum=momentum, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_log_scale(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use trained Encoder part to get the features for train, test and evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_fno = extract_latent_features_mae(trained_model, dataloader_train, device)\n",
    "test_features, test_fno = extract_latent_features_mae(trained_model, dataloader_test, device)\n",
    "eval_features, eval_fno = extract_latent_features_mae(trained_model, dataloader_eval, device)\n",
    "\n",
    "train_features = train_features.cpu()\n",
    "test_features = test_features.cpu()\n",
    "eval_features = eval_features.cpu()\n",
    "\n",
    "combined_train_features = torch.cat((train_features, test_features), dim=0)\n",
    "combined_train_coords = train_fno + test_fno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = train_kmeans_patches(combined_train_features, n_clusters=2, random_state=103)\n",
    "\n",
    "train_patch_predictions = kmeans.predict(combined_train_features.reshape(combined_train_features.size(0), -1).numpy().astype(np.float32))\n",
    "eval_patch_predictions = kmeans.predict(eval_features.reshape(eval_features.size(0), -1).numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_patch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, precision, recall, f1, f2 = get_clustering_accuracy(field_numbers_eval, eval_patch_predictions, config.labels_path)\n",
    "print(\"Accuracy:\",acc)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"F1-score:\",f1)\n",
    "print(\"F2-score:\",f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(trained_model, dataloader_train, device, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE functions --- tweeked for our usecase :) --- remove later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patchify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def patchify(self, images, patch_size=4):\n",
    "    \"\"\" images: (N,C,H,W)\n",
    "        patches(x): (N, L, patch_size ** 2 * C) \n",
    "        valid_patch_mask: (N, L)\n",
    "    \"\"\"\n",
    "\n",
    "    N, C, H, W = images.shape\n",
    "    p = patch_size\n",
    "    # p = self.patch_embed.patch_size[0]\n",
    "\n",
    "    assert H % p == 0 and W % p == 0, \"Image dimensions must be divisible by patch_size\"\n",
    "    \n",
    "    h = w = H // p\n",
    "    print(h,w)\n",
    "    x = images.reshape(shape = (N, C, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(N, h * w, p**2 * C))\n",
    "\n",
    "    # Create valid patch mask (1 for non-zero patches, 0 for black patches)\n",
    "    # A black patch is a patch with all zeros, so check if the sum of the patch is zero\n",
    "    valid_patch_mask = (x.sum(dim=-1) != 0).float()  # Shape: (B, num_patches)\n",
    "\n",
    "    return x , valid_patch_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_temporal_stack_rgb(images_visualisation_train[0],acquisition_dates_train[field_numbers_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_tensor[:,0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchified, valid_patch_mask = patchify(0, patch_tensor[:,0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_patch_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-patchify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(x):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 * C)\n",
    "    imgs: (N, C, H, W)\n",
    "    Dont need to handle valid_patch_mask because we wont be sending invalid patches to the encoder and decoder.. \n",
    "    So at the end we only have valid patches to unpatchify..\n",
    "    \"\"\"\n",
    "    print(x.shape)\n",
    "    N, L, O = x.shape\n",
    "    # p = patch_size\n",
    "    p = 4\n",
    "    print(p)\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    print(h,w)\n",
    "    assert h * w == x.shape[1]\n",
    "\n",
    "    C = O // p**2\n",
    "    x = x.reshape(N, h, w, p, p, C)\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "\n",
    "    images = x.reshape(N, C, h*p, h*p)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpatchified_images = unpatchify(patchified)\n",
    "unpatchified_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpatchified_images_r = unpatchified_images.permute(0,2,3,1)\n",
    "unpatchified_images_s = normalize_for_display(unpatchified_images_r)\n",
    "plt.imshow(unpatchified_images_s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_masking_with_max(self, x, mask_ratio, valid_patch_mask, mask=None):\n",
    "    \"\"\" Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffiling is done by argsort random noise.\n",
    "        x: [N,L,D], sequence\n",
    "        Here, we completely discard the black patches, and use only valid patches as input for- masking and shuffing ..\n",
    "    \"\"\"\n",
    "\n",
    "    N, L, D = x.shape  # Batch, length, p^2 * C\n",
    "    # len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "    valid_patch_mask = valid_patch_mask.bool()\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2425, 256, 48], [2425, 256]\n",
    "\n",
    "    # Remove samples with no valid patches\n",
    "    valid_samples = valid_patch_mask.sum(dim=1) > 0  \n",
    "    x = x[valid_samples]\n",
    "    valid_patch_mask = valid_patch_mask[valid_samples]\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2345, 256, 48], [2345, 256]\n",
    "\n",
    "    x_valid_list = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        valid_patches = x[i][valid_patch_mask[i]]\n",
    "        x_valid_list.append(valid_patches)\n",
    "\n",
    "    # max_valid_patches = max([valid_patch_mask[i].sum().item() for i in range(x.shape[0])])\n",
    "    max_valid_patches = 200      # take as hyperparam later\n",
    "    print(max_valid_patches)\n",
    "\n",
    "    # Pad each sample's valid patches to the max number of valid patches\n",
    "    padded_x_valid_list = []\n",
    "    for valid_patches in x_valid_list:\n",
    "\n",
    "        avg_value = valid_patches.mean(dim=0)       # Average along the patch dimension (dim=0)\n",
    "        avg_value_scalar = avg_value.mean().item()  \n",
    "        padding_size = max_valid_patches - valid_patches.shape[0]\n",
    "        # padded_valid_patches = F.pad(valid_patches, (0, 0, 0, padding_size), value=avg_value_scalar)\n",
    "        padded_valid_patches = F.pad(valid_patches, (0, 0, 0, padding_size), value=0)\n",
    "        padded_x_valid_list.append(padded_valid_patches)\n",
    "\n",
    "    padded_x_valid = torch.stack(padded_x_valid_list)\n",
    "    print(padded_x_valid.shape)      # [2325, max_valid_patches, 48]\n",
    "\n",
    "    # Update variables\n",
    "    N = padded_x_valid.shape[0]  \n",
    "    L = max_valid_patches\n",
    "    len_keep = int(L * (1 - mask_ratio))\n",
    "    # noise = torch.randn(N, max_valid_patches, device=padded_x_valid.device)  # Use updated N\n",
    "    noise = torch.randn(N, L, device=padded_x_valid.device)  # Noise [0,1]\n",
    "\n",
    "    # if self.same_mask:\n",
    "    if True:\n",
    "\n",
    "        while L % 4 != 0:\n",
    "            L += 1\n",
    "\n",
    "        L2 = L // 4             # 4 components (Original code had 3 components, but we keep 4)\n",
    "        assert 4 * L2 == L\n",
    "        noise = torch.randn(N, L2, device=padded_x_valid.device)  # Noise [0,1] for L2 tokens\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)    # Shuffling\n",
    "        ids_shuffle = [ids_shuffle + i * L2 for i in range(3)]\n",
    "\n",
    "        ids_shuffle_keep = [z[: ,:int(L2 * (1 - mask_ratio))] for z in ids_shuffle]     # To Keep\n",
    "        ids_shuffle_disc = [z[: ,int(L2 * (1 - mask_ratio)):] for z in ids_shuffle]     # To Mask\n",
    "        ids_shuffle = []\n",
    "        for z in ids_shuffle_keep:\n",
    "            ids_shuffle.append(z)\n",
    "        for z in ids_shuffle_disc:\n",
    "            ids_shuffle.append(z)\n",
    "        ids_shuffle = torch.cat(ids_shuffle, dim=1)\n",
    "\n",
    "    else:\n",
    "        if mask is None:\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)   # if no mask, noise small=keep, noise large=remove\n",
    "        else:\n",
    "            ids_shuffle = mask\n",
    "    \n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1) # For restoring the unshuffled version\n",
    "\n",
    "    ids_keep = ids_shuffle[:,:len_keep]             # Keep 1st subset\n",
    "    x_masked = torch.gather(padded_x_valid, dim=1, index=ids_keep.unsqueeze(-1).repeat(1,1,D))\n",
    "\n",
    "    # Binary mask: 0 = keep, 1 = remove\n",
    "    mask = torch.ones([N, L], device=padded_x_valid.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)      # Unshuffle to get the binary mask\n",
    "\n",
    "    return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_masking(self, x, mask_ratio, valid_patch_mask, mask=None):\n",
    "    \"\"\" Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffiling is done by argsort random noise.\n",
    "        x: [N,L,D], sequence\n",
    "        Here, we completely discard the black patches, and use only valid patches as input for- masking and shuffing ..\n",
    "    \"\"\"\n",
    "\n",
    "    N, L, D = x.shape  # Batch, length, p^2 * C\n",
    "    len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "    valid_patch_mask = valid_patch_mask.bool()\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2425, 256, 48], [2425, 256]\n",
    "\n",
    "    # Remove samples with no valid patches\n",
    "    valid_samples = valid_patch_mask.sum(dim=1) > 0  \n",
    "    x = x[valid_samples]\n",
    "    valid_patch_mask = valid_patch_mask[valid_samples]\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2345, 256, 48], [2345, 256]\n",
    "\n",
    "    # Update variables\n",
    "    N = x.shape[0]  \n",
    "    noise = torch.randn(N, L, device=x.device)  # Noise [0,1]\n",
    "\n",
    "    # if self.same_mask:\n",
    "    if True:\n",
    "\n",
    "        while L % 4 != 0:\n",
    "            L += 1\n",
    "\n",
    "        L2 = L // 4             # 4 components (Original code had 3 components, but we keep 4)\n",
    "        assert 4 * L2 == L\n",
    "        noise = torch.randn(N, L2, device=x.device)  # Noise [0,1] for L2 tokens\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)    # Shuffling\n",
    "        ids_shuffle = [ids_shuffle + i * L2 for i in range(3)]\n",
    "\n",
    "        ids_shuffle_keep = [z[: ,:int(L2 * (1 - mask_ratio))] for z in ids_shuffle]     # To Keep\n",
    "        ids_shuffle_disc = [z[: ,int(L2 * (1 - mask_ratio)):] for z in ids_shuffle]     # To Mask\n",
    "        ids_shuffle = []\n",
    "        for z in ids_shuffle_keep:\n",
    "            ids_shuffle.append(z)\n",
    "        for z in ids_shuffle_disc:\n",
    "            ids_shuffle.append(z)\n",
    "        ids_shuffle = torch.cat(ids_shuffle, dim=1)\n",
    "\n",
    "    else:\n",
    "        if mask is None:\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)   # if no mask, noise small=keep, noise large=remove\n",
    "        else:\n",
    "            ids_shuffle = mask\n",
    "    \n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1) # For restoring the unshuffled version\n",
    "\n",
    "    ids_keep = ids_shuffle[:,:len_keep]             # Keep 1st subset\n",
    "    x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1,1,D))\n",
    "\n",
    "    # Binary mask: 0 = keep, 1 = remove\n",
    "    mask = torch.ones([N, L], device=x.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)      # Unshuffle to get the binary mask\n",
    "\n",
    "    return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_masked, mask, ids_restore = random_masking(0, patchified, 0.75, valid_patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_restore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_patch_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_dates_eval['1168663.0_1176271.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Acquisition Dates Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_train = process_timestamps(field_numbers, acquisition_dates)\n",
    "timestamps_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_dates['1167134.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_numbers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(self, x, timestamps, mask_ratio, mask=None):\n",
    "    \n",
    "    \n",
    "    # Patch Embeddings for all 3 temporal images\n",
    "    x1 = self.patch_embed(x[:, 0])\n",
    "    x2 = self.patch_embed(x[:, 1])\n",
    "    x3 = self.patch_embed(x[:, 2])\n",
    "    x = torch.cat([x1, x2, x3], dim=1)\n",
    "    \n",
    "    # Temporal Embeddings\n",
    "    print(timestamps.shape, x.shape)\n",
    "    ts_embed = torch.cat([get_1d_sincos_pos_embed_from_grid_torch(128, timestamps.reshape(-1, 3)[:, 0].float()),\n",
    "                get_1d_sincos_pos_embed_from_grid_torch(128, timestamps.reshape(-1, 3)[:, 1].float()),\n",
    "                get_1d_sincos_pos_embed_from_grid_torch(128, timestamps.reshape(-1, 3)[:, 2].float())], dim=1).float()\n",
    "    \n",
    "    # ts_embed = torch.cat([\n",
    "    #     get_1d_sincos_pos_embed_from_grid_torch(128, timestamps[:, :, 0].reshape(-1, 1)),  # Year\n",
    "    #     get_1d_sincos_pos_embed_from_grid_torch(128, timestamps[:, :, 1].reshape(-1, 1)),  # Month\n",
    "    #     get_1d_sincos_pos_embed_from_grid_torch(128, timestamps[:, :, 2].reshape(-1, 1))   # Day\n",
    "    # ], dim=1).float()\n",
    "    \n",
    "    print(ts_embed, ts_embed.shape)\n",
    "    \n",
    "    ts_embed = ts_embed.reshape(-1, 3, ts_embed.shape[-1]).unsqueeze(2)\n",
    "    print(ts_embed.shape)\n",
    "    ts_embed = ts_embed.expand(-1, -1, x.shape[1] // 3, -1).reshape(x.shape[0], -1, ts_embed.shape[-1])\n",
    "    print(ts_embed.shape)\n",
    "\n",
    "    # Add positional embedding without cls token\n",
    "    x = x + torch.cat([self.pos_embed[:, 1:, :].repeat(ts_embed.shape[0], 3, 1), ts_embed], dim=-1)\n",
    "\n",
    "    # Masking: length -> length * mask_ratio\n",
    "    x, mask, ids_restore = self.random_masking(x, mask_ratio, mask=mask)\n",
    "\n",
    "    # Append cls token\n",
    "    cls_token = self.cls_token #+ self.pos_embed[:, :1, :]\n",
    "    cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)\n",
    "    x = self.norm(x)\n",
    "\n",
    "    return x, mask, ids_restore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_decoder(self, x, timestamps, ids_restore):\n",
    "        \n",
    "        # Decoder Embeddings for tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # Append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)                                       # No cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # Unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)                                                 # Append cls token\n",
    "\n",
    "\n",
    "        ts_embed = torch.cat([get_1d_sincos_pos_embed_from_grid_torch(64, timestamps.reshape(-1, 3)[:, 0].float()),\n",
    "                   get_1d_sincos_pos_embed_from_grid_torch(64, timestamps.reshape(-1, 3)[:, 1].float()),\n",
    "                   get_1d_sincos_pos_embed_from_grid_torch(64, timestamps.reshape(-1, 3)[:, 2].float())], dim=1).float()\n",
    "        \n",
    "        ts_embed = ts_embed.reshape(-1, 3, ts_embed.shape[-1]).unsqueeze(2)\n",
    "        ts_embed = ts_embed.expand(-1, -1, x.shape[1] // 3, -1).reshape(x.shape[0], -1, ts_embed.shape[-1])\n",
    "\n",
    "        ts_embed = torch.cat([torch.zeros((ts_embed.shape[0], 1, ts_embed.shape[2]), device=ts_embed.device), ts_embed], dim=1)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + torch.cat(\n",
    "            [torch.cat([self.decoder_pos_embed[:, :1, :], self.decoder_pos_embed[:, 1:, :].repeat(1, 3, 1)], dim=1).expand(ts_embed.shape[0], -1, -1),\n",
    "             ts_embed], dim=-1)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # Remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(self, images, pred, mask):\n",
    "        \"\"\"\n",
    "        images: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove\n",
    "        \"\"\"\n",
    "        target1 = self.patchify(images[:, 0])\n",
    "        target2 = self.patchify(images[:, 1])\n",
    "        target3 = self.patchify(images[:, 2])\n",
    "        target = torch.cat([target1, target2, target3], dim=1)\n",
    "        previous_target = target\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        # viz code\n",
    "        '''\n",
    "        m = torch.tensor([0.4182007312774658, 0.4214799106121063, 0.3991275727748871]).reshape(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.28774282336235046, 0.27541765570640564, 0.2764017581939697]).reshape(1, 3, 1, 1)\n",
    "        \n",
    "        image = (pred * (var + 1.e-6)**.5) + mean\n",
    "        bs = image.shape[0]\n",
    "        image = image.reshape(bs, 3, -1, image.shape[-1])[0]\n",
    "        image = self.unpatchify(image).detach().cpu()\n",
    "        image = image * std + m\n",
    "\n",
    "        save_image(image, f'viz1/viz_{self.counter}.png')\n",
    "        masked_image = self.patchify(image)\n",
    "        masked_image.reshape(-1, 768)[mask[0].bool()] = 0.5\n",
    "        masked_image = self.unpatchify(masked_image.reshape(3, -1 ,768))\n",
    "        save_image(masked_image, f'viz1/viz_mask_{self.counter}.png')\n",
    "\n",
    "        previous_target = previous_target.reshape(bs, 3, -1, previous_target.shape[-1])[0]\n",
    "        previous_target = self.unpatchify(previous_target).detach().cpu()\n",
    "        previous_target = previous_target * std + m\n",
    "        save_image(previous_target, f'viz1/target_{self.counter}.png')\n",
    "\n",
    "        masked_image = self.patchify(previous_target)\n",
    "        masked_image.reshape(-1, 768)[mask[0].bool()] = 0.5\n",
    "        masked_image = self.unpatchify(masked_image.reshape(3, -1 ,768))\n",
    "        save_image(masked_image, f'viz1/viz_target_mask_{self.counter}.png')\n",
    "        # print(image.shape)\n",
    "        # assert False\n",
    "        self.counter += 1\n",
    "        '''\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)                 # [N, L], mean loss per patch\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "def forward(self, imgs, timestamps, mask_ratio=0.75, mask=None):\n",
    "    latent, mask, ids_restore = self.forward_encoder(imgs, timestamps, mask_ratio, mask=mask)\n",
    "    pred = self.forward_decoder(latent, timestamps, ids_restore)  # [N, L, p*p*3]\n",
    "    loss = self.forward_loss(imgs, pred, mask)\n",
    "    return loss, pred, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Functions Together in a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --old-- POS encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "N = 4  # Batch size\n",
    "T = 7  # Number of temporal images (7 acquisition dates)\n",
    "C = 10  # Number of channels\n",
    "H = 64  # Image height\n",
    "W = 64  # Image width\n",
    "\n",
    "images = torch.randn(N, T, C, H, W)  # Example batch of satellite images\n",
    "temporal_indices = torch.arange(T).unsqueeze(0).expand(N, -1)  # Example temporal indices\n",
    "\n",
    "# Create model\n",
    "model = MAEViT(embed_dim=256, num_heads=8, ff_dim=1024, num_layers=6, num_patches=256, patch_size=16, img_size=64, num_channels=10)\n",
    "\n",
    "# Forward pass\n",
    "reconstructed_images = model(images, temporal_indices)\n",
    "\n",
    "print(f\"Reconstructed Images shape: {reconstructed_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
