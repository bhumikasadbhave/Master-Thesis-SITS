{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "sys.path.append('/home/k64835/Master-Thesis-SITS')\n",
    "\n",
    "scripts_path = Path(\"../Data-Preprocessing/\").resolve()\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "scripts_path = Path(\"../Evaluation/\").resolve()\n",
    "sys.path.append(str(scripts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k64835/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/k64835/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/k64835/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from scripts.data_visualiser import *\n",
    "from sklearn.manifold import TSNE \n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_scripts.subpatch_extraction import *\n",
    "from scripts.data_loader import *\n",
    "from scripts.data_preprocessor import *\n",
    "from scripts.temporal_data_preprocessor import *\n",
    "from scripts.temporal_data_loader import *\n",
    "from scripts.temporal_visualiser import *\n",
    "from scripts.temporal_chanel_refinement import *\n",
    "from model_scripts.model_helper import *\n",
    "from model_scripts.dataset_creation import *\n",
    "from model_scripts.train_model_ae import *\n",
    "from model_scripts.model_visualiser import *\n",
    "from model_scripts.clustering import *\n",
    "from model_scripts.train_model_dcec import *\n",
    "from model_scripts.pos_embed import *\n",
    "from model_scripts.train_mae import *\n",
    "from model_scripts.MaskedAutoencoderViT import *\n",
    "from evaluation_scripts.evaluation_helper import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "from evaluation_scripts.result_visualiser import *\n",
    "from Pipeline.temporal_preprocessing_pipeline import *\n",
    "import numpy as np\n",
    "import datetime\n",
    "import config as config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep: B10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-processed data\n",
    "\n",
    "Data: Extracted and Pre-processed Patches (each patch containing a sugarbeet field)\n",
    "\n",
    "Dimensions: (N, T, C, H, W) = (N, 3, 3, 64, 64)\n",
    "We take 3 temporal images: 1 from July, 1 from August and 1 from September"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2425, 3, 3, 64, 64]), torch.Size([48, 3, 3, 64, 64]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = PreProcessingPipelineTemporal()\n",
    "field_numbers, acquisition_dates, patch_tensor, images_visualisation_train = preprocessing_pipeline.get_processed_temporal_cube3('train', 'rgb')\n",
    "field_numbers_eval, acquisition_dates_eval, patch_tensor_eval, images_visualisation_eval = preprocessing_pipeline.get_processed_temporal_cube3('eval', 'rgb')\n",
    "patch_tensor.shape, patch_tensor_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2425"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps = process_timestamps(field_numbers, acquisition_dates)\n",
    "eval_timestamps = process_timestamps(field_numbers_eval, acquisition_dates_eval)\n",
    "len(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k64835/Master-Thesis-SITS/Modeling/model_scripts/dataset_creation.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).permute(0, 2, 1, 3, 4)   # (N, T, C, H, W) -> (N, C, T, H, W)\n"
     ]
    }
   ],
   "source": [
    "train_tensor, test_tensor, train_field_numbers, test_field_numbers, timestamps_train, timestamps_test = train_test_split(\n",
    "    patch_tensor, field_numbers, timestamps, test_size=1-config.ae_train_test_ratio, random_state=42\n",
    ")\n",
    "\n",
    "dataloader_train = create_data_loader_mae(train_tensor, train_field_numbers, timestamps_train, batch_size=config.ae_batch_size, shuffle=True)\n",
    "dataloader_test = create_data_loader_mae(test_tensor, test_field_numbers, timestamps_test, batch_size=config.ae_batch_size, shuffle=False)\n",
    "dataloader_eval = create_data_loader_mae(patch_tensor_eval, field_numbers_eval, acquisition_dates_eval, batch_size=config.ae_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Inputs Shape: torch.Size([64, 3, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch_inputs, batch_field_numbers, timestamps in dataloader_train:\n",
    "    print(\"Batch Inputs Shape:\", batch_inputs.shape)  \n",
    "    # print(\"Batch Field Numbers:\", batch_field_numbers)\n",
    "    # print(\"Dates:\", timestamps.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE functions -- handmade :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patchify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def patchify(self, images, patch_size=4):\n",
    "    \"\"\" images: (N,C,H,W)\n",
    "        patches(x): (N, L, patch_size ** 2 * C) \n",
    "        valid_patch_mask: (N, L)\n",
    "    \"\"\"\n",
    "\n",
    "    N, C, H, W = images.shape\n",
    "    p = patch_size\n",
    "    # p = self.patch_embed.patch_size[0]\n",
    "\n",
    "    assert H % p == 0 and W % p == 0, \"Image dimensions must be divisible by patch_size\"\n",
    "    \n",
    "    h = w = H // p\n",
    "    x = images.reshape(shape = (N, C, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(N, h * w, p**2 * C))\n",
    "\n",
    "    # Create valid patch mask (1 for non-zero patches, 0 for black patches)\n",
    "    # A black patch is a patch with all zeros, so check if the sum of the patch is zero\n",
    "    valid_patch_mask = (x.sum(dim=-1) != 0).float()  # Shape: (B, num_patches)\n",
    "\n",
    "    return x , valid_patch_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_temporal_stack_rgb(images_visualisation_train[0],acquisition_dates_train[field_numbers_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2425, 3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_tensor[:,0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchified, valid_patch_mask = patchify(0, patch_tensor[:,0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2425, 256, 48])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2425, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_patch_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-patchify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(self, x, patch_size):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 * C)\n",
    "    imgs: (N, C, H, W)\n",
    "    Dont need to handle valid_patch_mask because we wont be sending invalid patches to the encoder and decoder.. \n",
    "    So at the end we only have valid patches to unpatchify..\n",
    "    \"\"\"\n",
    "    N, L, O = x.shape\n",
    "    p = patch_size\n",
    "    # p = self.patch_embed.patch_size[0]\n",
    "    h = w = int(L**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "\n",
    "    C = O // patch_size**2\n",
    "    x = x.reshape(N, h, w, p, p, C)\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "\n",
    "    images = x.reshape(N, C, h*p, h*p)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2425, 3, 64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpatchified_images = unpatchify(0, patchified, 4)\n",
    "unpatchified_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_masking_with_max(self, x, mask_ratio, valid_patch_mask, mask=None):\n",
    "    \"\"\" Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffiling is done by argsort random noise.\n",
    "        x: [N,L,D], sequence\n",
    "        Here, we completely discard the black patches, and use only valid patches as input for- masking and shuffing ..\n",
    "    \"\"\"\n",
    "\n",
    "    N, L, D = x.shape  # Batch, length, p^2 * C\n",
    "    # len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "    valid_patch_mask = valid_patch_mask.bool()\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2425, 256, 48], [2425, 256]\n",
    "\n",
    "    # Remove samples with no valid patches\n",
    "    valid_samples = valid_patch_mask.sum(dim=1) > 0  \n",
    "    x = x[valid_samples]\n",
    "    valid_patch_mask = valid_patch_mask[valid_samples]\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2345, 256, 48], [2345, 256]\n",
    "\n",
    "    x_valid_list = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        valid_patches = x[i][valid_patch_mask[i]]\n",
    "        x_valid_list.append(valid_patches)\n",
    "\n",
    "    # max_valid_patches = max([valid_patch_mask[i].sum().item() for i in range(x.shape[0])])\n",
    "    max_valid_patches = 200      # take as hyperparam later\n",
    "    print(max_valid_patches)\n",
    "\n",
    "    # Pad each sample's valid patches to the max number of valid patches\n",
    "    padded_x_valid_list = []\n",
    "    for valid_patches in x_valid_list:\n",
    "\n",
    "        avg_value = valid_patches.mean(dim=0)       # Average along the patch dimension (dim=0)\n",
    "        avg_value_scalar = avg_value.mean().item()  \n",
    "        padding_size = max_valid_patches - valid_patches.shape[0]\n",
    "        # padded_valid_patches = F.pad(valid_patches, (0, 0, 0, padding_size), value=avg_value_scalar)\n",
    "        padded_valid_patches = F.pad(valid_patches, (0, 0, 0, padding_size), value=0)\n",
    "        padded_x_valid_list.append(padded_valid_patches)\n",
    "\n",
    "    padded_x_valid = torch.stack(padded_x_valid_list)\n",
    "    print(padded_x_valid.shape)      # [2325, max_valid_patches, 48]\n",
    "\n",
    "    # Update variables\n",
    "    N = padded_x_valid.shape[0]  \n",
    "    L = max_valid_patches\n",
    "    len_keep = int(L * (1 - mask_ratio))\n",
    "    # noise = torch.randn(N, max_valid_patches, device=padded_x_valid.device)  # Use updated N\n",
    "    noise = torch.randn(N, L, device=padded_x_valid.device)  # Noise [0,1]\n",
    "\n",
    "    # if self.same_mask:\n",
    "    if True:\n",
    "\n",
    "        while L % 4 != 0:\n",
    "            L += 1\n",
    "\n",
    "        L2 = L // 4             # 4 components (Original code had 3 components, but we keep 4)\n",
    "        assert 4 * L2 == L\n",
    "        noise = torch.randn(N, L2, device=padded_x_valid.device)  # Noise [0,1] for L2 tokens\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)    # Shuffling\n",
    "        ids_shuffle = [ids_shuffle + i * L2 for i in range(3)]\n",
    "\n",
    "        ids_shuffle_keep = [z[: ,:int(L2 * (1 - mask_ratio))] for z in ids_shuffle]     # To Keep\n",
    "        ids_shuffle_disc = [z[: ,int(L2 * (1 - mask_ratio)):] for z in ids_shuffle]     # To Mask\n",
    "        ids_shuffle = []\n",
    "        for z in ids_shuffle_keep:\n",
    "            ids_shuffle.append(z)\n",
    "        for z in ids_shuffle_disc:\n",
    "            ids_shuffle.append(z)\n",
    "        ids_shuffle = torch.cat(ids_shuffle, dim=1)\n",
    "\n",
    "    else:\n",
    "        if mask is None:\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)   # if no mask, noise small=keep, noise large=remove\n",
    "        else:\n",
    "            ids_shuffle = mask\n",
    "    \n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1) # For restoring the unshuffled version\n",
    "\n",
    "    ids_keep = ids_shuffle[:,:len_keep]             # Keep 1st subset\n",
    "    x_masked = torch.gather(padded_x_valid, dim=1, index=ids_keep.unsqueeze(-1).repeat(1,1,D))\n",
    "\n",
    "    # Binary mask: 0 = keep, 1 = remove\n",
    "    mask = torch.ones([N, L], device=padded_x_valid.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)      # Unshuffle to get the binary mask\n",
    "\n",
    "    return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_masking(self, x, mask_ratio, valid_patch_mask, mask=None):\n",
    "    \"\"\" Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffiling is done by argsort random noise.\n",
    "        x: [N,L,D], sequence\n",
    "        Here, we completely discard the black patches, and use only valid patches as input for- masking and shuffing ..\n",
    "    \"\"\"\n",
    "\n",
    "    N, L, D = x.shape  # Batch, length, p^2 * C\n",
    "    len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "    valid_patch_mask = valid_patch_mask.bool()\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2425, 256, 48], [2425, 256]\n",
    "\n",
    "    # Remove samples with no valid patches\n",
    "    valid_samples = valid_patch_mask.sum(dim=1) > 0  \n",
    "    x = x[valid_samples]\n",
    "    valid_patch_mask = valid_patch_mask[valid_samples]\n",
    "    print(x.shape, valid_patch_mask.shape)          # [2345, 256, 48], [2345, 256]\n",
    "\n",
    "    # Update variables\n",
    "    N = x.shape[0]  \n",
    "    noise = torch.randn(N, L, device=x.device)  # Noise [0,1]\n",
    "\n",
    "    # if self.same_mask:\n",
    "    if True:\n",
    "\n",
    "        while L % 4 != 0:\n",
    "            L += 1\n",
    "\n",
    "        L2 = L // 4             # 4 components (Original code had 3 components, but we keep 4)\n",
    "        assert 4 * L2 == L\n",
    "        noise = torch.randn(N, L2, device=x.device)  # Noise [0,1] for L2 tokens\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)    # Shuffling\n",
    "        ids_shuffle = [ids_shuffle + i * L2 for i in range(3)]\n",
    "\n",
    "        ids_shuffle_keep = [z[: ,:int(L2 * (1 - mask_ratio))] for z in ids_shuffle]     # To Keep\n",
    "        ids_shuffle_disc = [z[: ,int(L2 * (1 - mask_ratio)):] for z in ids_shuffle]     # To Mask\n",
    "        ids_shuffle = []\n",
    "        for z in ids_shuffle_keep:\n",
    "            ids_shuffle.append(z)\n",
    "        for z in ids_shuffle_disc:\n",
    "            ids_shuffle.append(z)\n",
    "        ids_shuffle = torch.cat(ids_shuffle, dim=1)\n",
    "\n",
    "    else:\n",
    "        if mask is None:\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)   # if no mask, noise small=keep, noise large=remove\n",
    "        else:\n",
    "            ids_shuffle = mask\n",
    "    \n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1) # For restoring the unshuffled version\n",
    "\n",
    "    ids_keep = ids_shuffle[:,:len_keep]             # Keep 1st subset\n",
    "    x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1,1,D))\n",
    "\n",
    "    # Binary mask: 0 = keep, 1 = remove\n",
    "    mask = torch.ones([N, L], device=x.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)      # Unshuffle to get the binary mask\n",
    "\n",
    "    return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2425, 256, 48]) torch.Size([2425, 256])\n",
      "torch.Size([2345, 256, 48]) torch.Size([2345, 256])\n"
     ]
    }
   ],
   "source": [
    "x_masked, mask, ids_restore = random_masking(0, patchified, 0.75, valid_patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2345, 64, 48])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2345, 192])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2345, 192])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_restore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2425, 256])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_patch_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190600.0',\n",
       " '20190608.0',\n",
       " '20190700.0',\n",
       " '20190716.0',\n",
       " '20190818.0',\n",
       " '20190824.0',\n",
       " '20190904.0']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquisition_dates_eval['1168663.0_1176271.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Acquisition Dates Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2425, 3, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps_train = process_timestamps(field_numbers, acquisition_dates)\n",
    "timestamps_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190604.0',\n",
       " '20190620.0',\n",
       " '20190704.0',\n",
       " '20190724.0',\n",
       " '20190818.0',\n",
       " '20190824.0',\n",
       " '20190902.0']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquisition_dates['1167134.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1167134.0'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_numbers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(self, x, timestamps, mask_ratio, mask=None):\n",
    "    \n",
    "    \n",
    "    # Patch Embeddings for all 3 temporal images\n",
    "    x1 = self.patch_embed(x[:, 0])\n",
    "    x2 = self.patch_embed(x[:, 1])\n",
    "    x3 = self.patch_embed(x[:, 2])\n",
    "    x = torch.cat([x1, x2, x3], dim=1)\n",
    "    \n",
    "    # Temporal Embeddings\n",
    "    print(timestamps.shape, x.shape)\n",
    "    ts_embed = torch.cat([get_1d_sincos_pos_embed_from_grid_torch(128, timestamps.reshape(-1, 3)[:, 0].float()),\n",
    "                get_1d_sincos_pos_embed_from_grid_torch(128, timestamps.reshape(-1, 3)[:, 1].float()),\n",
    "                get_1d_sincos_pos_embed_from_grid_torch(128, timestamps.reshape(-1, 3)[:, 2].float())], dim=1).float()\n",
    "    \n",
    "    # ts_embed = torch.cat([\n",
    "    #     get_1d_sincos_pos_embed_from_grid_torch(128, timestamps[:, :, 0].reshape(-1, 1)),  # Year\n",
    "    #     get_1d_sincos_pos_embed_from_grid_torch(128, timestamps[:, :, 1].reshape(-1, 1)),  # Month\n",
    "    #     get_1d_sincos_pos_embed_from_grid_torch(128, timestamps[:, :, 2].reshape(-1, 1))   # Day\n",
    "    # ], dim=1).float()\n",
    "    \n",
    "    print(ts_embed, ts_embed.shape)\n",
    "    \n",
    "    ts_embed = ts_embed.reshape(-1, 3, ts_embed.shape[-1]).unsqueeze(2)\n",
    "    print(ts_embed.shape)\n",
    "    ts_embed = ts_embed.expand(-1, -1, x.shape[1] // 3, -1).reshape(x.shape[0], -1, ts_embed.shape[-1])\n",
    "    print(ts_embed.shape)\n",
    "\n",
    "    # Add positional embedding without cls token\n",
    "    x = x + torch.cat([self.pos_embed[:, 1:, :].repeat(ts_embed.shape[0], 3, 1), ts_embed], dim=-1)\n",
    "\n",
    "    # Masking: length -> length * mask_ratio\n",
    "    x, mask, ids_restore = self.random_masking(x, mask_ratio, mask=mask)\n",
    "\n",
    "    # Append cls token\n",
    "    cls_token = self.cls_token #+ self.pos_embed[:, :1, :]\n",
    "    cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)\n",
    "    x = self.norm(x)\n",
    "\n",
    "    return x, mask, ids_restore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_decoder(self, x, timestamps, ids_restore):\n",
    "        \n",
    "        # Decoder Embeddings for tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # Append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)                                       # No cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # Unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)                                                 # Append cls token\n",
    "\n",
    "\n",
    "        ts_embed = torch.cat([get_1d_sincos_pos_embed_from_grid_torch(64, timestamps.reshape(-1, 3)[:, 0].float()),\n",
    "                   get_1d_sincos_pos_embed_from_grid_torch(64, timestamps.reshape(-1, 3)[:, 1].float()),\n",
    "                   get_1d_sincos_pos_embed_from_grid_torch(64, timestamps.reshape(-1, 3)[:, 2].float())], dim=1).float()\n",
    "        \n",
    "        ts_embed = ts_embed.reshape(-1, 3, ts_embed.shape[-1]).unsqueeze(2)\n",
    "        ts_embed = ts_embed.expand(-1, -1, x.shape[1] // 3, -1).reshape(x.shape[0], -1, ts_embed.shape[-1])\n",
    "\n",
    "        ts_embed = torch.cat([torch.zeros((ts_embed.shape[0], 1, ts_embed.shape[2]), device=ts_embed.device), ts_embed], dim=1)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + torch.cat(\n",
    "            [torch.cat([self.decoder_pos_embed[:, :1, :], self.decoder_pos_embed[:, 1:, :].repeat(1, 3, 1)], dim=1).expand(ts_embed.shape[0], -1, -1),\n",
    "             ts_embed], dim=-1)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # Remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(self, images, pred, mask):\n",
    "        \"\"\"\n",
    "        images: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove\n",
    "        \"\"\"\n",
    "        target1 = self.patchify(images[:, 0])\n",
    "        target2 = self.patchify(images[:, 1])\n",
    "        target3 = self.patchify(images[:, 2])\n",
    "        target = torch.cat([target1, target2, target3], dim=1)\n",
    "        previous_target = target\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        # viz code\n",
    "        '''\n",
    "        m = torch.tensor([0.4182007312774658, 0.4214799106121063, 0.3991275727748871]).reshape(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.28774282336235046, 0.27541765570640564, 0.2764017581939697]).reshape(1, 3, 1, 1)\n",
    "        \n",
    "        image = (pred * (var + 1.e-6)**.5) + mean\n",
    "        bs = image.shape[0]\n",
    "        image = image.reshape(bs, 3, -1, image.shape[-1])[0]\n",
    "        image = self.unpatchify(image).detach().cpu()\n",
    "        image = image * std + m\n",
    "\n",
    "        save_image(image, f'viz1/viz_{self.counter}.png')\n",
    "        masked_image = self.patchify(image)\n",
    "        masked_image.reshape(-1, 768)[mask[0].bool()] = 0.5\n",
    "        masked_image = self.unpatchify(masked_image.reshape(3, -1 ,768))\n",
    "        save_image(masked_image, f'viz1/viz_mask_{self.counter}.png')\n",
    "\n",
    "        previous_target = previous_target.reshape(bs, 3, -1, previous_target.shape[-1])[0]\n",
    "        previous_target = self.unpatchify(previous_target).detach().cpu()\n",
    "        previous_target = previous_target * std + m\n",
    "        save_image(previous_target, f'viz1/target_{self.counter}.png')\n",
    "\n",
    "        masked_image = self.patchify(previous_target)\n",
    "        masked_image.reshape(-1, 768)[mask[0].bool()] = 0.5\n",
    "        masked_image = self.unpatchify(masked_image.reshape(3, -1 ,768))\n",
    "        save_image(masked_image, f'viz1/viz_target_mask_{self.counter}.png')\n",
    "        # print(image.shape)\n",
    "        # assert False\n",
    "        self.counter += 1\n",
    "        '''\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)                 # [N, L], mean loss per patch\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "def forward(self, imgs, timestamps, mask_ratio=0.75, mask=None):\n",
    "    latent, mask, ids_restore = self.forward_encoder(imgs, timestamps, mask_ratio, mask=mask)\n",
    "    pred = self.forward_decoder(latent, timestamps, ids_restore)  # [N, L, p*p*3]\n",
    "    loss = self.forward_loss(imgs, pred, mask)\n",
    "    return loss, pred, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Functions Together in a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE Script -------> Runnnnnnnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "momentum=0.9\n",
    "latent_dim=32\n",
    "channels = 3\n",
    "time_steps = 3\n",
    "optimizer = 'Adam'\n",
    "patch_size = config.subpatch_size\n",
    "masking_ratio = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedAutoencoderViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-7): 8 x Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=512, out_features=48, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MaskedAutoencoderViT(\n",
    "        img_size=64,\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 64, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model, train_losses, test_losses  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasking_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasking_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Master-Thesis-SITS/Modeling/model_scripts/train_mae.py:51\u001b[0m, in \u001b[0;36mtrain_model_mae\u001b[0;34m(model, train_dataloader, test_dataloader, epochs, masking_ratio, optimizer, lr, momentum, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 51\u001b[0m pred, mask, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasking_ratio\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Master-Thesis-SITS/Modeling/model_scripts/MaskedAutoencoderViT.py:298\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward\u001b[0;34m(self, imgs, timestamps, mask_ratio, mask)\u001b[0m\n\u001b[1;32m    296\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x1, x2, x3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    297\u001b[0m valid_patch_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([valid_mask1, valid_mask2, valid_mask3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 298\u001b[0m latent, mask, ids_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_patch_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# latent, mask, ids_restore = self.forward_encoder(imgs, timestamps, mask_ratio, mask=mask)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_decoder(latent, timestamps, ids_restore)  \u001b[38;5;66;03m# [N, L, p*p*3]\u001b[39;00m\n",
      "File \u001b[0;32m~/Master-Thesis-SITS/Modeling/model_scripts/MaskedAutoencoderViT.py:185\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward_encoder\u001b[0;34m(self, x, timestamps, mask_ratio, valid_patch_mask, mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, timestamps, mask_ratio, valid_patch_mask, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    183\u001b[0m     \n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Patch Embeddings for all 3 temporal images\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    187\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x[:, \u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/layers/patch_embed.py:41\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 41\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     42\u001b[0m     _assert(H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image height (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     _assert(W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image width (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "trained_model, train_losses, test_losses  = train_model_mae(model, dataloader_train, dataloader_test, epochs=epochs, masking_ratio=masking_ratio, optimizer=optimizer, lr=lr, momentum=momentum, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --old-- POS encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got input (256), mat (256x2), vec (128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m temporal_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(T)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Example temporal indices\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMAEViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m reconstructed_images \u001b[38;5;241m=\u001b[39m model(images, temporal_indices)\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mMAEViT.__init__\u001b[0;34m(self, embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(MAEViT, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mViTEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m ViTDecoder(embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels)\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mViTEncoder.__init__\u001b[0;34m(self, embed_dim, num_heads, ff_dim, num_layers, num_patches, patch_size, img_size, num_channels, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(num_channels, embed_dim, kernel_size\u001b[38;5;241m=\u001b[39mpatch_size, stride\u001b[38;5;241m=\u001b[39mpatch_size)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Positional encoding\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Transformer encoder layers\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     16\u001b[0m     [ViTBlock(embed_dim, num_heads, ff_dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, num_patches, embed_dim, max_len)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28msuper\u001b[39m(PositionalEncoding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Spatial Positional Encoding (2D)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_pos_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_2d_positional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Temporal Positional Encoding (1D)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_pos_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_1d_positional_encoding(max_len, embed_dim)\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mPositionalEncoding.get_2d_positional_encoding\u001b[0;34m(self, num_patches, embed_dim)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply sine/cosine function\u001b[39;00m\n\u001b[1;32m     25\u001b[0m angle_rates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m10000\u001b[39m, (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, embed_dim, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()) \u001b[38;5;241m/\u001b[39m embed_dim)\n\u001b[0;32m---> 26\u001b[0m angle_rads \u001b[38;5;241m=\u001b[39m \u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mangle_rates\u001b[49m  \u001b[38;5;66;03m# (1, num_patches, embed_dim // 2)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply sin and cos functions\u001b[39;00m\n\u001b[1;32m     29\u001b[0m pos_encoding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39msin(angle_rads), torch\u001b[38;5;241m.\u001b[39mcos(angle_rads)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got input (256), mat (256x2), vec (128)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "N = 4  # Batch size\n",
    "T = 7  # Number of temporal images (7 acquisition dates)\n",
    "C = 10  # Number of channels\n",
    "H = 64  # Image height\n",
    "W = 64  # Image width\n",
    "\n",
    "images = torch.randn(N, T, C, H, W)  # Example batch of satellite images\n",
    "temporal_indices = torch.arange(T).unsqueeze(0).expand(N, -1)  # Example temporal indices\n",
    "\n",
    "# Create model\n",
    "model = MAEViT(embed_dim=256, num_heads=8, ff_dim=1024, num_layers=6, num_patches=256, patch_size=16, img_size=64, num_channels=10)\n",
    "\n",
    "# Forward pass\n",
    "reconstructed_images = model(images, temporal_indices)\n",
    "\n",
    "print(f\"Reconstructed Images shape: {reconstructed_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
